{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.pipeline.wrapper import Module, dsl\n",
    "from azureml.pipeline.wrapper._dataset import get_global_dataset_by_path\n",
    "from external_sub_pipeline import external_sub_pipeline0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module\n",
    "execute_python_script_module = Module.load(ws, namespace='azureml', name='Execute Python Script')\n",
    "\n",
    "\n",
    "# TODO: Dataset\n",
    "blob_input_data = get_global_dataset_by_path(ws, 'Automobile_price_data', 'GenericCSV/Automobile_price_data_(Raw)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_name = 'aml_module_training_data'\n",
    "\n",
    "if training_data_name not in ws.datasets:\n",
    "    print('Registering a training dataset for sample pipeline ...')\n",
    "    train_data = Dataset.File.from_files(path=['https://dprepdata.blob.core.windows.net/demo/Titanic.csv'])\n",
    "    train_data.register(workspace=ws,\n",
    "                        name=training_data_name,\n",
    "                        description='Training data (just for illustrative purpose)')\n",
    "    print('Registerd')\n",
    "else:\n",
    "    train_data = ws.datasets[training_data_name]\n",
    "    print('Training dataset found in workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='sub0 graph', description='sub0')\n",
    "def sub_pipeline0(input):\n",
    "    module1 = execute_python_script_module(\n",
    "        # should be pipeline input\n",
    "        dataset1=input,\n",
    "    )\n",
    "    module2 = execute_python_script_module(\n",
    "        dataset1=module1.outputs.result_dataset,\n",
    "    )\n",
    "    return module2.outputs\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='sub1 graph', description='sub1')\n",
    "def sub_pipeline1(input):\n",
    "    module1 = execute_python_script_module(\n",
    "        dataset1=input\n",
    "    )\n",
    "    sub0 = sub_pipeline0(module1.outputs.result_dataset)\n",
    "    return sub0.outputs\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='sub2 graph', description='sub1')\n",
    "def sub_pipeline2(input):\n",
    "    module1 = execute_python_script_module(\n",
    "        dataset1=input\n",
    "    )\n",
    "    module2 = execute_python_script_module(\n",
    "        dataset1=module1.outputs.result_dataset,\n",
    "        dataset2=blob_input_data\n",
    "    )\n",
    "    module3 = execute_python_script_module(\n",
    "        dataset1=input,\n",
    "        dataset2=module2.outputs.result_dataset\n",
    "    )\n",
    "    module4 = execute_python_script_module(\n",
    "        dataset1=train_data,\n",
    "        dataset2=module3.outputs.result_dataset\n",
    "    )\n",
    "    sub0 = sub_pipeline0(module4.outputs.result_dataset)\n",
    "    return sub0.outputs\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='parent graph', description='parent', default_compute_target=\"aml-compute\")\n",
    "def parent_pipeline():\n",
    "    @dsl.pipeline(name='internal sub graph', description='internal sub')\n",
    "    def sub_pipeline_internal(input):\n",
    "        module1 = execute_python_script_module(\n",
    "            # should be pipeline input\n",
    "            dataset1=input,\n",
    "        )\n",
    "        module2 = execute_python_script_module(\n",
    "            dataset1=module1.outputs.result_dataset,\n",
    "        )\n",
    "        return module2.outputs\n",
    "\n",
    "    sub0 = sub_pipeline_internal(blob_input_data)\n",
    "    sub1 = sub_pipeline1(sub0.outputs.result_dataset)\n",
    "    sub2 = sub_pipeline2(sub1.outputs.result_dataset)\n",
    "    module2 = execute_python_script_module(\n",
    "        dataset1=sub2.outputs.result_dataset,\n",
    "        dataset2=train_data,\n",
    "    )\n",
    "\n",
    "    external = external_sub_pipeline0(sub1.outputs.result_dataset)\n",
    "    return module2.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = parent_pipeline()\n",
    "pipeline1.validate()\n",
    "\n",
    "run = pipeline1.submit(\n",
    "    experiment_name='module_SDK_test'\n",
    ")\n",
    "run.wait_for_completion()\n",
    "\n",
    "pipeline1.save(\n",
    "    experiment_name='module_SDK_test'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "clwan"
   }
  ],
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python (aml)",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}